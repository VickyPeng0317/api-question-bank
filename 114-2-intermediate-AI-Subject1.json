{
  "level": "intermediate",
  "type": "人工智慧技術應用與規劃",
  "quiz_id": "114-2-AI-Subject1",
  "title": "114年第二梯次中級AI應用規劃師第一科：人工智慧技術應用與規劃",
  "exam_date": "2025-11-08",
  "questions": [
    {
      "id": "q1",
      "type": "single_choice",
      "questionText": "某電商企業希望利用自然語言處理（NLP）技術，分析顧客在社群平台與商品評論中的文字內容，以即時掌握顧客對產品的滿意度變化。若採用情感分析（Sentiment Analysis）模型，其主要目的為何？",
      "options": [
        { "key": "A", "value": "預測顧客使用的語言風格與語氣" },
        { "key": "B", "value": "判斷文本中所表達的情感傾向" },
        { "key": "C", "value": "將顧客留言自動翻譯成企業內部指定語言" },
        { "key": "D", "value": "產生顧客評論的自動化摘要內容" }
      ],
      "answer": "B",
      "explanation": "情感分析（Sentiment Analysis）的核心任務是識別文本中的主觀情緒，例如正面、負面或中性，以判斷顧客的滿意度傾向。",
      "score": 2,
      "tags": ["NLP", "情感分析"]
    },
    {
      "id": "q2",
      "type": "single_choice",
      "questionText": "某跨國金融科技公司導入 Transformer架構開發多語客服系統，以提升長篇金融文件的自動翻譯品質。下列何者為該模型能顯著改善翻譯準確度的主要原因？",
      "options": [
        { "key": "A", "value": "透過自注意力機制（Self-Attention Mechanism）捕捉長距離語境依賴關係" },
        { "key": "B", "value": "透過卷積運算（Convolution Operation）加速訓練過程" },
        { "key": "C", "value": "透過強化學習（Reinforcement Learning）自動調整語句生成策略" },
        { "key": "D", "value": "透過資料增強（Data Augmentation）平衡多語語料比例" }
      ],
      "answer": "A",
      "explanation": "Transformer 的核心優勢在於自注意力機制（Self-Attention），它能讓模型在處理每一個詞時，同時關注序列中距離較遠的相關詞彙，有效解決長文本的依賴問題。",
      "score": 2,
      "tags": ["Transformer", "NLP"]
    },
    {
      "id": "q3",
      "type": "single_choice",
      "questionText": "某企業計畫應用 BERT（Bidirectional Encoder Representations from Transformers）模型分析大量顧客意見，以強化客服自動回覆系統。在 BERT 的預訓練過程中，「遮罩語言模型（Masked Language Model, MLM）」的主要訓練策略為何？",
      "options": [
        { "key": "A", "value": "依序遮罩句尾詞語，讓模型從左到右逐步生成完整句子" },
        { "key": "B", "value": "隨機遮罩部分詞語，並讓模型根據雙向上下文（Bidirectional Context）預測被遮罩的詞" },
        { "key": "C", "value": "透過對抗訓練（Adversarial Training）生成語意相似的擾動樣本以提升泛化性" },
        { "key": "D", "value": "以未遮罩的詞為條件，使用解碼器（Decoder）結構重建整句內容" }
      ],
      "answer": "B",
      "explanation": "BERT 使用 MLM 任務，隨機遮蓋輸入序列中約 15% 的詞彙，並要求模型根據前後文（雙向）來預測這些被遮蓋的詞，從而學習深層語意。",
      "score": 2,
      "tags": ["BERT", "預訓練模型"]
    },
    {
      "id": "q4",
      "type": "single_choice",
      "questionText": "在詞向量（Word Embedding）訓練方法中，GloVe（Global Vectors for Word Representation）與 Word2Vec的主要差異為何？",
      "options": [
        { "key": "A", "value": "Word2Vec 以詞頻權重訓練詞向量，而 GloVe以隨機初始化向量進行學習" },
        { "key": "B", "value": "Word2Vec 以全局統計矩陣為基礎，而 GloVe採用神經網路進行上下文預測" },
        { "key": "C", "value": "Word2Vec 為基於預測的模型，而 GloVe為基於共現統計的模型" },
        { "key": "D", "value": "Word2Vec 僅能用於靜態文本語料，而 GloVe可應用於即時語料更新" }
      ],
      "answer": "C",
      "explanation": "Word2Vec（如 Skip-gram/CBOW）是透過預測上下文來學習詞向量（Predictive）；GloVe 則是透過分解全局詞共現矩陣來學習（Count-based/Statistical）。",
      "score": 2,
      "tags": ["NLP", "Word Embedding"]
    },
    {
      "id": "q5",
      "type": "single_choice",
      "questionText": "某企業以詞頻–逆文件頻率（Term Frequency–Inverse Document Frequency, TF-IDF）方法分析顧客意見內容，但發現模型在處理篇幅較長的回饋文本時，無法準確反映關鍵詞的重要性。下列何者為造成此現象的主要原因？",
      "options": [
        { "key": "A", "value": "長文本中的詞頻偏高，導致常見詞權重被過度放大" },
        { "key": "B", "value": "長文本中缺乏明確句子邊界，造成 TF-IDF 無法計算詞頻" },
        { "key": "C", "value": "TF-IDF 無法同時處理多份文件" },
        { "key": "D", "value": "長文本會改變 IDF（Inverse Document Frequency）的計算，使所有詞權重趨於相近" }
      ],
      "answer": "A",
      "explanation": "在未正規化的情況下，長文本中即使是一般詞彙出現次數也會變多（TF變高），這可能導致常見詞的權重被不當放大，掩蓋了真正關鍵詞的重要性。",
      "score": 2,
      "tags": ["NLP", "特徵工程"]
    },
    {
      "id": "q6",
      "type": "single_choice",
      "questionText": "某企業嘗試以 N-gram語言模型（N-gram Language Model）建立客服自動回覆系統，但發現模型生成的句子雖在片段上合理，卻缺乏整體語意連貫性。此問題最可能源自 N-gram模型的哪一項限制？",
      "options": [
        { "key": "A", "value": "N-gram 模型在訓練過程中需要龐大計算量，導致長句無法收斂" },
        { "key": "B", "value": "N-gram 模型僅根據固定長度的前序詞建立機率估計，難以捕捉長距離依賴關係（Long-range Dependencies）" },
        { "key": "C", "value": "N-gram 模型缺乏語意嵌入（Semantic Embedding）層，因此無法表徵詞語間的語意相似度" },
        { "key": "D", "value": "N-gram 模型假設詞與詞之間相互獨立，導致無法建構上下文語意關聯" }
      ],
      "answer": "B",
      "explanation": "N-gram 模型僅依賴前 N-1 個詞來預測下一個詞（馬可夫假設），因此無法記憶更早之前的上下文，導致長句生成時容易失去連貫性。",
      "score": 2,
      "tags": ["NLP", "語言模型"]
    },
    {
      "id": "q7",
      "type": "single_choice",
      "questionText": "在企業導入的智慧監控系統中，模型以物件偵測（Object Detection）方式自動辨識影像中的人物與車輛。若評估指標採用平均精確率（Mean Average Precision, mAP），其中 IoU（Intersection over Union）閾值設定較高時，代表下列哪一項意義？",
      "options": [
        { "key": "A", "value": "預測邊界框與真實邊界框的重疊程度越高，模型偵測結果越精準" },
        { "key": "B", "value": "預測邊界框與真實邊界框的誤差越大，導致 mAP數值上升" },
        { "key": "C", "value": "模型整體精確率（Precision）降低，但召回率（Recall）上升" },
        { "key": "D", "value": "預測邊界框的評估結果不受真實框大小影響" }
      ],
      "answer": "A",
      "explanation": "IoU 閾值越高（例如 0.75 或 0.9），表示系統認定「正確偵測」的標準越嚴格，要求預測框必須與真實框高度重疊。",
      "score": 2,
      "tags": ["電腦視覺", "物件偵測"]
    },
    {
      "id": "q8",
      "type": "single_choice",
      "questionText": "關於 Softmax 與 Max-Pooling，下列敘述何者正確？",
      "options": [
        { "key": "A", "value": "Softmax 與 Max-Pooling都會將特徵張量壓縮為單一最大值" },
        { "key": "B", "value": "Max-Pooling 會對輸入進行機率分佈的轉換" },
        { "key": "C", "value": "Softmax 會保留所有輸入資訊，但以比例表示；Max-Pooling 只保留區域最大值" },
        { "key": "D", "value": "Softmax 主要用於特徵降維，而 Max-Pooling用於分類輸出" }
      ],
      "answer": "C",
      "explanation": "Softmax 將數值轉換為總和為 1 的機率分佈（保留所有資訊）；Max-Pooling 則是取區域內的最大值（降維並丟棄其他資訊）。",
      "score": 2,
      "tags": ["深度學習", "CNN"]
    },
    {
      "id": "q9",
      "type": "single_choice",
      "questionText": "某企業在訓練生成式 AI模型時，導入資料增強（Data Augmentation）技術以擴充訓練資料，但觀察到模型效能反而下降。下列哪一項最可能的原因與對應改善策略最為正確？",
      "options": [
        { "key": "A", "value": "增強樣本未經隨機初始化，導致模型梯度更新不穩定，應重新設計訓練啟動流程" },
        { "key": "B", "value": "增強後資料的特徵分佈與原始資料不一致，影響模型的泛化能力，應檢查並調整增強策略以維持語意一致性" },
        { "key": "C", "value": "增強樣本的比例過高，造成模型對特定資料產生偏好，應適度提高增強比例並調整學習率" },
        { "key": "D", "value": "增強後資料的標註可信度下降，導致訓練訊號偏差，應以半監督學習方式重新校正資料" }
      ],
      "answer": "B",
      "explanation": "資料增強必須保持數據的語意不變。若增強操作（如過度旋轉、裁切或錯誤的同義詞替換）破壞了資料特徵分佈，會導致模型學到錯誤模式，降低效能。",
      "score": 2,
      "tags": ["資料增強", "模型訓練"]
    },
    {
      "id": "q10",
      "type": "single_choice",
      "questionText": "如果希望同時兼顧「精確率（Precision）」和「召回率（Recall）」，下列哪一個指標可以作為綜合評估的標準？",
      "options": [
        { "key": "A", "value": "準確率（Accuracy）" },
        { "key": "B", "value": "均方根誤差（RMSE）" },
        { "key": "C", "value": "均方誤差（MSE）" },
        { "key": "D", "value": "F1分數（F1 Score）" }
      ],
      "answer": "D",
      "explanation": "F1 Score 是 Precision 與 Recall 的調和平均數，專門用於評估兩者之間的平衡，特別適合資料不平衡的情境。",
      "score": 2,
      "tags": ["模型評估", "F1 Score"]
    },
    {
      "id": "q11",
      "type": "single_choice",
      "questionText": "企業資料分析團隊使用 DBSCAN（Density-Based Spatial Clustering of Applications with Noise）演算法進行顧客行為分群，並希望模型能自動區分主要群集與雜訊資料。在此演算法中，決定聚類結果的兩個主要超參數為下列何者？",
      "options": [
        { "key": "A", "value": "特徵數與學習率" },
        { "key": "B", "value": "K 值與距離閾值" },
        { "key": "C", "value": "鄰域半徑（Epsilon ε）與最小點數（MinPts）" },
        { "key": "D", "value": "交叉熵（Cross Entropy）與權重初始化" }
      ],
      "answer": "C",
      "explanation": "DBSCAN 的核心參數是 Epsilon（定義鄰域範圍）和 MinPts（定義核心點所需的最小鄰居數），這兩者決定了密度的定義與群集的形成。",
      "score": 2,
      "tags": ["分群", "DBSCAN"]
    },
    {
      "id": "q12",
      "type": "single_choice",
      "questionText": "某金融科技公司建立房價預測模型，使用多項特徵（如建坪、房齡、樓層、總價等）進行線性迴歸分析（Linear Regression Analysis）。資料分析師發現多個特徵之間存在高度相關性，導致模型係數不穩定、預測誤差上升。為解決此問題，下列哪一種方法最適合？",
      "options": [
        { "key": "A", "value": "繼續保留所有特徵，不進行任何處理" },
        { "key": "B", "value": "使用主成分分析（PCA）將相關特徵轉換為彼此獨立的主成分" },
        { "key": "C", "value": "新增更多原始變數以提升模型表現" },
        { "key": "D", "value": "改用分類模型進行預測" }
      ],
      "answer": "B",
      "explanation": "多重共線性（Multicollinearity）會影響迴歸模型。主成分分析（PCA）可以將相關的高維特徵轉換為一組線性獨立的主成分，從而消除共線性問題。",
      "score": 2,
      "tags": ["特徵工程", "PCA"]
    },
    {
      "id": "q13",
      "type": "single_choice",
      "questionText": "下列何者為 Kubernetes在 AI模型部署與運行中的核心功能？",
      "options": [
        { "key": "A", "value": "自動化管理模型的訓練流程與參數調校" },
        { "key": "B", "value": "管理與協調模型服務的部署、擴展與運行環境" },
        { "key": "C", "value": "提供 AI 模型的資料儲存與版本控管功能" },
        { "key": "D", "value": "負責深度學習推論的 GPU 加速運算" }
      ],
      "answer": "B",
      "explanation": "Kubernetes 是容器編排平台，核心功能是自動化部署、擴展（Scaling）與管理容器化應用程式（如 AI 模型服務），而非直接負責訓練調校或 GPU 運算。",
      "score": 2,
      "tags": ["MLOps", "Kubernetes"]
    },
    {
      "id": "q14",
      "type": "single_choice",
      "questionText": "在調整模型超參數（Hyperparameters）時，若希望避免因過度調整參數而導致過擬合，下列哪一種做法最有效提升模型的泛化能力？",
      "options": [
        { "key": "A", "value": "採用交叉驗證（Cross-Validation）於多組參數組合間反覆評估，選擇在驗證資料上表現最穩定的設定" },
        { "key": "B", "value": "使用早期停止機制（Early Stopping）監控訓練誤差並在收斂前停止訓練，以防模型學習過度" },
        { "key": "C", "value": "對輸入特徵進行標準化以減少特徵值差異帶來的過擬合風險" },
        { "key": "D", "value": "提高模型複雜度並使用更多超參數搜尋範圍，以確保模型能充分學習資料特徵" }
      ],
      "answer": "A",
      "explanation": "交叉驗證（Cross-Validation）能確保模型參數不是僅針對單一驗證集優化，而是對不同資料子集都有效，從而提升泛化能力並減少過擬合。",
      "score": 2,
      "tags": ["模型優化", "交叉驗證"]
    },
    {
      "id": "q15",
      "type": "single_choice",
      "questionText": "在企業導入的 MLOps（Machine Learning Operations）流程中，Model Registry 最常用於哪一個階段？",
      "options": [
        { "key": "A", "value": "用於設定運算資源與執行環境以確保訓練穩定" },
        { "key": "B", "value": "用於建立可重複使用的資料與特徵版本" },
        { "key": "C", "value": "用於集中管理模型版本、訓練紀錄與部署狀態" },
        { "key": "D", "value": "用於追蹤模型上線後的表現與漂移情況" }
      ],
      "answer": "C",
      "explanation": "Model Registry 是 MLOps 的核心組件，專門用於註冊、版本控制、標記（Staging/Production）及管理模型的生命週期。",
      "score": 2,
      "tags": ["MLOps", "Model Registry"]
    },
    {
      "id": "q16",
      "type": "single_choice",
      "questionText": "下列哪一種情境中最適合使用「序列到序列（Seq2Seq）」模型？",
      "options": [
        { "key": "A", "value": "預測銷售趨勢曲線，輸出未來數值序列" },
        { "key": "B", "value": "辨識文本中出現的人名、地名與組織名稱等實體資訊" },
        { "key": "C", "value": "對輸入文本中的關鍵字進行頻率統計與可視化" },
        { "key": "D", "value": "將輸入文字轉換成語意等價的另一段文字，如自動翻譯或摘要生成" }
      ],
      "answer": "D",
      "explanation": "Seq2Seq 模型（如 Encoder-Decoder 架構）最經典的應用就是輸入一段序列並輸出另一段序列，例如機器翻譯（中翻英）或文章摘要。",
      "score": 2,
      "tags": ["NLP", "Seq2Seq"]
    },
    {
      "id": "q17",
      "type": "single_choice",
      "questionText": "在自然語言處理中，檢索增強生成（Retrieval-Augmented Generation, RAG）是一種結合語言模型與向量搜尋的技術，可有效減少模型知識過時與產生幻覺的問題。若要建立一套高效能的 RAG系統，下列何者為在「檢索階段」最關鍵的挑戰？",
      "options": [
        { "key": "A", "value": "確保檢索到的文件能被完整納入語言模型的上下文視窗（Context Window）中進行生成" },
        { "key": "B", "value": "選擇使用 Faiss或 ScaNN等近似最近鄰搜尋函式庫" },
        { "key": "C", "value": "降低嵌入模型（Embedding Model）在高維空間中的計算成本與記憶體占用" },
        { "key": "D", "value": "避免向量檢索結果僅具語意相似但與查詢意圖無實質關聯的情況" }
      ],
      "answer": "D",
      "explanation": "向量檢索基於語意相似度，但「相似」不等於「相關」或「正確」。例如查詢「蘋果」，可能會檢索到水果，但用戶意圖可能是科技公司，這種意圖不匹配是檢索階段的最大挑戰。",
      "score": 2,
      "tags": ["RAG", "資訊檢索"]
    },
    {
      "id": "q18",
      "type": "single_choice",
      "questionText": "當 Transformer 模型發生「注意力分布過於平均（Attention Collapse）」的情形時，導致模型無法有效聚焦於關鍵資訊，下列哪一項策略可有效改善此問題？",
      "options": [
        { "key": "A", "value": "提高 Query-Key點積（Dot Product）的縮放常數" },
        { "key": "B", "value": "在 Softmax 前加入高斯雜訊（Gaussian Noise）" },
        { "key": "C", "value": "使用 ReLU 函數取代 Softmax" },
        { "key": "D", "value": "對注意力權重施加稀疏化約束（Sparsity Constraint）" }
      ],
      "answer": "D",
      "explanation": "Attention Collapse 表示模型對所有輸入都給予相似的權重。施加稀疏化約束（Sparsity Constraint）能強迫模型只關注最重要的部分，從而恢復注意力的聚焦能力。",
      "score": 2,
      "tags": ["Transformer", "Attention機制"]
    },
    {
      "id": "q19",
      "type": "single_choice",
      "questionText": "某研究團隊正在訓練一個針對低資源語言（如少數民族語言）的語言模型，但該語言僅有約 1萬筆語料可用。在訓練過程中出現明顯的過擬合現象，若希望在不新增真實語料的前提下提升模型的泛化能力，採用下列哪一種方法最為適合？",
      "options": [
        { "key": "A", "value": "將 Transformer 的隱藏層維度擴增至 1024，以提升表徵能力" },
        { "key": "B", "value": "採用反向翻譯（Back-Translation）技術，以生成額外目標語句的偽平行語料（Pseudo‑Parallel Corpus）" },
        { "key": "C", "value": "對詞嵌入矩陣（Embedding Matrix），施加 L1正則化以壓縮模型參數" },
        { "key": "D", "value": "將多語言 BERT（mBERT）中所有 Transformer層全部凍結以保留預訓練知識" }
      ],
      "answer": "B",
      "explanation": "反向翻譯（Back-Translation）是低資源語言常見的資料增強技術，透過將目標語言翻譯成來源語言再翻譯回來，生成偽資料來擴充訓練集。",
      "score": 2,
      "tags": ["NLP", "資料增強"]
    },
    {
      "id": "q20",
      "type": "single_choice",
      "questionText": "在使用生成對抗網路（GAN）進行人臉影像生成時，若出現「模式崩潰」（Mode Collapse）現象，下列哪一種方法最常被用來有效解決此問題？",
      "options": [
        { "key": "A", "value": "在鑑別器中加入梯度懲罰（Gradient Penalty）以穩定訓練過程" },
        { "key": "B", "value": "採用 Wasserstein 距離（WGAN 損失）替代原始的 GAN 損失函數" },
        { "key": "C", "value": "對生成器輸入的潛在向量加入隨機擾動" },
        { "key": "D", "value": "使用多尺度鑑別器架構以提高對多樣性的判別能力" }
      ],
      "answer": "B",
      "explanation": "Mode Collapse 指生成器只能產生極少數種類的樣本。WGAN（Wasserstein GAN）引入了 Wasserstein 距離來衡量分佈差異，能有效解決梯度消失與模式崩潰問題。",
      "score": 2,
      "tags": ["GAN", "生成模型"]
    },
    {
      "id": "q21",
      "type": "single_choice",
      "questionText": "在多模態 AI 模型訓練或推論過程中，遇到某一模態資料缺失（例如僅有影像資料但缺少文本說明），下列哪一種策略最有效維持模型效能？",
      "options": [
        { "key": "A", "value": "以零向量或固定向量填充缺失模態輸入" },
        { "key": "B", "value": "訓練具備模態缺失感知能力的模型，使其適應缺失狀況" },
        { "key": "C", "value": "利用生成模型（如 GAN 或自迴歸模型）預測並補全缺失模態資料" },
        { "key": "D", "value": "直接捨棄缺少模態的樣本，避免干擾訓練或推論" }
      ],
      "answer": "B",
      "explanation": "與其依賴填補（可能引入雜訊）或捨棄（浪費資料），訓練一個能容忍缺失模態的模型（如使用 Dropout Modality 策略）能讓模型更具魯棒性。",
      "score": 2,
      "tags": ["多模態AI", "資料缺失"]
    },
    {
      "id": "q22",
      "type": "single_choice",
      "questionText": "某電商平台開發的顧客流失預測模型在上線數月後，預測準確率明顯下降。專案團隊懷疑顧客行為模式改變，導致模型輸入特徵的分佈與原始訓練資料不同，出現典型的資料漂移（Data Drift）問題。為了偵測並確認資料分佈是否發生變化，下列哪一種作法最合適？",
      "options": [
        { "key": "A", "value": "定期重新訓練模型以應對外部變化" },
        { "key": "B", "value": "提升模型複雜度以捕捉更多資料變異性" },
        { "key": "C", "value": "增加測試資料量以提高評估準確度" },
        { "key": "D", "value": "計算輸入特徵分佈間的 KL散度（KL Divergence）" }
      ],
      "answer": "D",
      "explanation": "KL 散度（Kullback-Leibler Divergence）或 PSI（Population Stability Index）是用來衡量兩個機率分佈（訓練集 vs. 上線數據）差異的標準統計指標，適合偵測漂移。",
      "score": 2,
      "tags": ["MLOps", "Data Drift"]
    },
    {
      "id": "q23",
      "type": "single_choice",
      "questionText": "某大型醫院即將部署一套輔助診斷的 AI系統，為降低對臨床流程的衝擊，同時確保風險可控與回饋可收斂，應採取何種『漸進式部署』（Phased Rollout）策略最為合適？",
      "options": [
        { "key": "A", "value": "從單一專科（如放射科）或特定病房開始啟用，逐步擴展至全院" },
        { "key": "B", "value": "先部署於病例量較高的急診單位，加速收集高頻使用回饋" },
        { "key": "C", "value": "僅在夜班或離峰時段啟用，避免影響主要臨床工作負載" },
        { "key": "D", "value": "在使用者界面啟用提示模式，讓全院同步體驗但不影響診斷流程" }
      ],
      "answer": "A",
      "explanation": "漸進式部署（Phased Rollout）強調從小範圍、可控的環境開始（如單一科室），確認穩定後再擴大，這能最小化風險。",
      "score": 2,
      "tags": ["AI部署", "風險管理"]
    },
    {
      "id": "q24",
      "type": "single_choice",
      "questionText": "某金融機構的 AI風控系統遭受對抗性攻擊，駭客透過對輸入特徵進行微小但惡意的擾動，成功欺騙了模型。為了從根本上解決模型自身對這類攻擊的脆弱性，下列何者並非針對此種攻擊型態的技術手段？",
      "options": [
        { "key": "A", "value": "強化資料前處理，用以過濾掉格式不符或數值極端異常的輸入" },
        { "key": "B", "value": "在模型訓練階段導入對抗樣本訓練，以提升模型對惡意特徵擾動的辨識與防禦能力" },
        { "key": "C", "value": "於推論後階段使用規則引擎，以確保模型的預測結果不違反既有的業務硬性規定" },
        { "key": "D", "value": "在模型部署環境中強化網路防火牆，以阻擋來自未授權來源的網路連線" }
      ],
      "answer": "D",
      "explanation": "對抗性攻擊（Adversarial Attack）是針對模型輸入內容的攻擊，防火牆（Firewall）只能阻擋網路層的未授權連線，無法檢測合法連線中的惡意資料內容。",
      "score": 2,
      "tags": ["AI安全", "對抗性攻擊"]
    },
    {
      "id": "q25",
      "type": "single_choice",
      "questionText": "某企業部署生成式 AI系統協助行銷與內容產出，但近期遭質疑部分生成內容可能涉及著作權侵權。為降低企業在法律層面的潛在責任與風險，下列哪一項策略最能有效預防侵權問題產生？",
      "options": [
        { "key": "A", "value": "對生成內容進行語意相似度比對，自動標註可能涉及既有著作的輸出結果，以降低侵權風險" },
        { "key": "B", "value": "建立訓練資料篩選與授權驗證機制，排除未授權或高風險資料來源" },
        { "key": "C", "value": "在訓練與微調過程中採用差分隱私技術，避免模型記憶特定受著作權保護的樣本" },
        { "key": "D", "value": "在模型輸出端嵌入浮水印（Watermarking）或數位指紋（Digital Fingerprint）技術，以確保生成內容可追溯" }
      ],
      "answer": "B",
      "explanation": "侵權問題的根源在於訓練資料。從源頭篩選並排除未授權的資料（Data Curation/Filtering）是預防侵權最根本且有效的手段。",
      "score": 2,
      "tags": ["AI法規", "著作權"]
    },
    {
      "id": "q26",
      "type": "single_choice",
      "questionText": "在房價預測任務中，若發現特徵如「房間數」與「坪數」存在高度多重共線性（Multicollinearity），為降低共線性對模型參數估計的負面影響，應優先選擇下列哪種模型？",
      "options": [
        { "key": "A", "value": "不受多重共線性影響的決策樹模型" },
        { "key": "B", "value": "傳統線性迴歸模型，不含正則化項" },
        { "key": "C", "value": "支持向量機搭配線性核函數" },
        { "key": "D", "value": "含 L1正則化的 LASSO迴歸模型" }
      ],
      "answer": "D",
      "explanation": "LASSO（L1 正則化）具備特徵選擇能力，能將共線性特徵中的部分係數壓縮至 0，僅保留一個代表性特徵，有效解決共線性問題。",
      "score": 2,
      "tags": ["機器學習", "LASSO"]
    },
    {
      "id": "q27",
      "type": "single_choice",
      "questionText": "某企業需分析半結構化的系統日誌（JSON格式），以提取關鍵的時序特徵供故障預測模型使用。考量日誌結構複雜且包含巢狀欄位（Nested Fields），下列哪一種策略最有效且實務可行？",
      "options": [
        { "key": "A", "value": "先將 JSON 資料扁平化轉成 CSV，再對欄位計算統計量（如均值、次數）作為特徵" },
        { "key": "B", "value": "使用遞歸神經網路（RNN）直接輸入原始 JSON字串進行時序特徵抽取" },
        { "key": "C", "value": "設計遞迴函式展開巢狀欄位，並基於時間窗口（Time Window）進行聚合與特徵萃取" },
        { "key": "D", "value": "只保留時間戳記欄位，忽略其他巢狀內容以簡化特徵工程" }
      ],
      "answer": "C",
      "explanation": "複雜 JSON 需要先展開（Flattening/Unnesting），然後針對時序資料的特性，利用時間窗口進行聚合（如計算每分鐘的錯誤次數），才能產生有意義的特徵。",
      "score": 2,
      "tags": ["資料處理", "特徵工程"]
    },
    {
      "id": "q28",
      "type": "single_choice",
      "questionText": "在一個同時包含連續型特徵與類別型特徵的資料集中，若希望透過適當的特徵工程流程來提升模型整體表現，下列哪一種作法最為合適？",
      "options": [
        { "key": "A", "value": "將類別型特徵使用標籤編碼（Label Encoding）轉換後，與連續特徵直接合併進行模型訓練" },
        { "key": "B", "value": "將連續特徵進行離散化（Discretization）或分桶（Binning）轉為類別型特徵，統一以類別方式處理" },
        { "key": "C", "value": "對連續特徵做標準化（Standardization），類別特徵採用目標編碼（Target Encoding），並生成交互特徵提升模型表現" },
        { "key": "D", "value": "只保留連續特徵，忽略類別型變量以簡化模型" }
      ],
      "answer": "C",
      "explanation": "標準化連續特徵能加速收斂；目標編碼（Target Encoding）能有效處理高基數類別特徵；交互特徵（Interaction Features）能捕捉非線性關係，綜合效果最佳。",
      "score": 2,
      "tags": ["特徵工程", "資料前處理"]
    },
    {
      "id": "q29",
      "type": "single_choice",
      "questionText": "某 AI開發團隊為提升模型開發效率及品質控制，計畫實施持續整合（Continuous Integration, CI）流程。下列哪一項做法最符合 CI的核心實踐，且能有效減少整合風險？",
      "options": [
        { "key": "A", "value": "在主分支（Main Branch）每日固定時間手動合併並執行完整測試流程" },
        { "key": "B", "value": "每次程式碼提交（Commit）後自動觸發建置、單元測試及靜態程式碼分析" },
        { "key": "C", "value": "於模型訓練完成後，定期安排開發團隊回顧並合併程式碼" },
        { "key": "D", "value": "透過自動化部署腳本，將模型在特定時間點批次釋出到測試環境" }
      ],
      "answer": "B",
      "explanation": "CI 的核心是「頻繁且自動化」。每次 Commit 即觸發自動測試，能即時發現錯誤，避免問題累積，降低整合風險。",
      "score": 2,
      "tags": ["MLOps", "CI/CD"]
    },
    {
      "id": "q30",
      "type": "single_choice",
      "questionText": "某銀行計劃將 AI詐欺偵測模組整合至核心交易系統，主管機關要求全流程必須符合金融監管對「不可否認性（Non-repudiation）」的資訊安全規範，以確保日後能進行法務追蹤與稽核。下列哪一項措施最能確保此要求的落實？",
      "options": [
        { "key": "A", "value": "為每筆 AI模型推論記錄其輸入與輸出結果的加密雜湊值（Hash），並簽署數位簽章以確保不可竄改性" },
        { "key": "B", "value": "優化模型效能以降低平均推論延遲至 100ms以下，提升使用者體檢" },
        { "key": "C", "value": "增加主機備援數量，以確保系統在故障時持續可用" },
        { "key": "D", "value": "將模型推論請求導入負載平衡器，避免單點壅塞導致服務延遲" }
      ],
      "answer": "A",
      "explanation": "不可否認性（Non-repudiation）需要證明「是誰做的」且「內容未被改過」。數位簽章（Digital Signature）結合雜湊值（Hash）是實現此目標的標準密碼學技術。",
      "score": 2,
      "tags": ["AI安全", "法規遵循"]
    },
    {
      "id": "q31",
      "type": "single_choice",
      "questionText": "某 AI服務系統每次推論請求需約 1秒完成，且必須支撐高達 10,000次請求每秒（RPS）的流量。為確保系統具備高可用性且能穩定應付流量峰值，下列哪一種架構方案最為合適？",
      "options": [
        { "key": "A", "value": "依賴單台超高效能伺服器進行垂直擴展，提升硬體規格" },
        { "key": "B", "value": "採用容器化部署並水平擴展服務實例，結合自動彈性伸縮機制（Auto Scaling）" },
        { "key": "C", "value": "限制最大併發連線數，以避免系統過載" },
        { "key": "D", "value": "增加批次處理大小，一次同時處理上千筆請求" }
      ],
      "answer": "B",
      "explanation": "高流量且需高可用性的場景，單機擴展（Vertical Scaling）有極限。水平擴展（Horizontal Scaling）配合自動伸縮（Auto Scaling）是雲端原生架構的最佳解。",
      "score": 2,
      "tags": ["系統架構", "Auto Scaling"]
    },
    {
      "id": "q32",
      "type": "single_choice",
      "questionText": "某企業已將 AI模型部署於生產環境，為確保系統持續穩定運作，並能提前偵測模型效能可能衰退，技術團隊希望透過監控指標進行預警。下列哪一項監控指標最具預測效力，能提早發現模型效能下滑風險？",
      "options": [
        { "key": "A", "value": "系統 CPU 與記憶體使用率波動幅度" },
        { "key": "B", "value": "模型推論結果的置信度（Confidence）分佈變化趨勢" },
        { "key": "C", "value": "API平均回應時間與延遲百分位數變化" },
        { "key": "D", "value": "輸入特徵與訓練資料分布差異的 PSI（Population Stability Index）指數" }
      ],
      "answer": "D",
      "explanation": "PSI（群體穩定性指標）專門用於量化訓練資料與實際上線資料的分佈差異。當 PSI 升高，代表資料漂移（Data Drift），預示模型效能即將或已經下降。",
      "score": 2,
      "tags": ["模型監控", "PSI"]
    },
    {
      "id": "q33",
      "type": "single_choice",
      "questionText": "企業團隊在使用 Word2Vec模型訓練客服文本語料時，若訓練資料量龐大且希望模型能更有效捕捉罕見詞的語意關聯，下列哪一種訓練策略最為適合？",
      "options": [
        { "key": "A", "value": "採用 Skip-gram模型，但以隨機初始化權重加快高頻詞的訓練收斂" },
        { "key": "B", "value": "採用 CBOW 模型（Continuous Bag of Words Model）並結合 TF-IDF權重以強化低頻詞表示" },
        { "key": "C", "value": "採用 Skip-gram模型，利用中心詞預測周圍詞語，能更有效學習低頻詞關係" },
        { "key": "D", "value": "採用 CBOW 模型（Continuous Bag of Words Model），利用周圍詞預測中心詞，能提升罕見詞的語意穩定度" }
      ],
      "answer": "C",
      "explanation": "Skip-gram 透過中心詞預測上下文，對於低頻詞（罕見詞）的學習效果通常優於 CBOW（CBOW 傾向平滑掉低頻詞）。",
      "score": 2,
      "tags": ["NLP", "Word2Vec"]
    },
    {
      "id": "q34",
      "type": "single_choice",
      "questionText": "在自駕車影像辨識系統中，開發團隊希望模型能同時辨識每個像素所屬的物件類別（例如道路、建築、行人），又能區分出同類物件的不同個體（例如多位行人）。此時最適合採用下列哪一項電腦視覺技術？",
      "options": [
        { "key": "A", "value": "語義分割（Semantic Segmentation）" },
        { "key": "B", "value": "物件偵測（Object Detection）" },
        { "key": "C", "value": "實例分割（Instance Segmentation）" },
        { "key": "D", "value": "全景分割（Panoptic Segmentation）" }
      ],
      "answer": "D",
      "explanation": "全景分割（Panoptic Segmentation）結合了語義分割（區分背景類別如道路）與實例分割（區分個體如行人A、行人B），滿足所有像素分類與個體區分的需求。",
      "score": 2,
      "tags": ["電腦視覺", "影像分割"]
    },
    {
      "id": "q35",
      "type": "single_choice",
      "questionText": "某媒體公司計畫導入 CLIP（Contrastive Language–Image Pre-training）模型，以協助大量影像自動標註與搜尋，並希望在無需新增標訓資料的情況下，僅透過文字提示（Text Prompt）即可識別影像內容。請問此應用情境中，CLIP能夠達成的關鍵技術特性為何？",
      "options": [
        { "key": "A", "value": "透過圖文對比式學習（Contrastive Learning）將影像與文字映射至共同嵌入空間（Shared Embedding Space），可直接以語意相似度進行零樣本分類" },
        { "key": "B", "value": "透過影像增強與特徵擴散降低標訓資料需求" },
        { "key": "C", "value": "以監督式學習結合多層感知器（Multilayer Perceptron, MLP）進行影像特徵分類" },
        { "key": "D", "value": "以自迴歸生成模型（Autoregressive Model）逐步生成文字標籤描述影像內容" }
      ],
      "answer": "A",
      "explanation": "CLIP 的核心是將圖片與文字映射到同一個向量空間，使其能計算彼此的相似度，從而實現 Zero-shot（零樣本）分類。",
      "score": 2,
      "tags": ["多模態AI", "CLIP"]
    },
    {
      "id": "q36",
      "type": "single_choice",
      "questionText": "某資料科學團隊在開發預測模型時，針對多種模型設定（如學習率、樹深度、正則化係數等）進行系統化測試，希望找出在驗證資料上表現最穩定的組合。此過程最可能採用下列哪一種方法？",
      "options": [
        { "key": "A", "value": "使用交叉驗證（Cross Validation）反覆評估模型以降低過擬合風險" },
        { "key": "B", "value": "透過網格搜尋（Grid Search）在多組超參數設定中進行系統化搜尋與評估" },
        { "key": "C", "value": "以隨機搜尋（Random Search）快速探索部分參數空間以提升搜尋效率" },
        { "key": "D", "value": "採用貝葉斯優化（Bayesian Optimization）根據歷次結果動態調整搜尋方向" }
      ],
      "answer": "B",
      "explanation": "「系統化測試」通常指網格搜尋（Grid Search），它會窮舉所有指定的參數組合，確保找到範圍內的最佳解。",
      "score": 2,
      "tags": ["模型優化", "超參數調整"]
    },
    {
      "id": "q37",
      "type": "single_choice",
      "questionText": "某公司正在訓練一個大型語音合成模型，開發團隊使用多台 GPU 進行訓練，但經常出現 GPU 記憶體不足問題。由於模型架構已固定且無法更換硬體，團隊希望在維持模型效能與收斂品質的前提下，下列哪一種方法最有效降低單張 GPU 的記憶體壓力？",
      "options": [
        { "key": "A", "value": "減少訓練資料量以降低記憶體使用" },
        { "key": "B", "value": "採用較小的批次大小（Batch Size）並搭配資料分片（Data Sharding）分散訓練負載" },
        { "key": "C", "value": "增加學習率（Learning Rate）以加快收斂速度" },
        { "key": "D", "value": "改用測試資料集（Test Set）進行部分訓練以節省空間" }
      ],
      "answer": "B",
      "explanation": "降低 Batch Size 直接減少記憶體佔用；資料分片（如 DDP）則將數據分散到不同 GPU，是解決 OOM（Out of Memory）的標準策略。",
      "score": 2,
      "tags": ["模型訓練", "硬體優化"]
    },
    {
      "id": "q38",
      "type": "single_choice",
      "questionText": "某影像設計團隊在使用 Stable Diffusion生成 4K級產品圖時，發現影像邊緣與細節存在顆粒化與模糊現象。若僅能在生成階段進行調整，希望提升畫面清晰度與紋理層次，同時避免過度平滑，下列哪一項作法最適合？",
      "options": [
        { "key": "A", "value": "降低取樣步數，以縮短生成時間" },
        { "key": "B", "value": "增加取樣步數並選擇高品質取樣器，以強化細節還原度" },
        { "key": "C", "value": "提高 CFG（Classifier-Free Guidance）值，使生成結果更具創意與多樣性" },
        { "key": "D", "value": "改用低解析度輸入以降低計算成本" }
      ],
      "answer": "B",
      "explanation": "增加取樣步數（Sampling Steps）能讓擴散模型進行更多次的去噪迭代，有助於生成更清晰、細節更豐富的圖像。",
      "score": 2,
      "tags": ["生成式AI", "Stable Diffusion"]
    },
    {
      "id": "q39",
      "type": "single_choice",
      "questionText": "某企業的資料科學團隊利用 ARIMA模型（AutoRegressive Integrated Moving Average Model）預測每週產品銷售量。模型建立完成後，分析人員發現預測誤差隨時間呈現週期性波動，且自相關函數（ACF）顯示殘差在多個時滯（Lag）上仍顯著不為零。根據上述現象，最合理的模型診斷結論為何？",
      "options": [
        { "key": "A", "value": "模型殘差符合白噪音（White Noise）假設，預測表現穩定" },
        { "key": "B", "value": "模型殘差雖有輕微異常，但可視為隨機誤差忽略不計" },
        { "key": "C", "value": "模型存在配適不足（Underfitting）問題，需重新調整 p 或 q 參數以捕捉時間依賴性" },
        { "key": "D", "value": "殘差特性不影響預測結果，無須進一步修正" }
      ],
      "answer": "C",
      "explanation": "殘差應為白噪音（無規律）。若殘差仍有顯著自相關或週期性，代表模型未能捕捉到數據中的所有資訊，屬於配適不足。",
      "score": 2,
      "tags": ["時間序列", "ARIMA"]
    },
    {
      "id": "q40",
      "type": "single_choice",
      "questionText": "下列哪一項最正確地描述了 VAE（Variational Autoencoder）、GAN（Generative Adversarial Network）與擴散模型（Diffusion Model）在多模態潛在空間對齊（Latent Alignment）與生成策略上的根本差異？",
      "options": [
        { "key": "A", "value": "VAE透過顯式潛在變數建模實現跨模態對齊，適合捕捉整體語意結構但生成解析度有限；GAN透過對抗損失（Adversarial Loss）在不同模態間學習分佈映射，生成品質高但穩定性差；擴散模型則以條件化噪聲反推（Conditional Denoising）方式實現高保真跨模態生成，兼具穩定性與多樣性" },
        { "key": "B", "value": "VAE與 Diffusion Ｍodel均屬隱式生成架構，主要依賴對抗式訓練實現跨模態對齊；GAN則以顯式後驗估計方式提升樣本一致性" },
        { "key": "C", "value": "VAE與 GAN 均使用馬爾可夫鏈（Markov Chain）進行跨模態轉換；Diffusion Model 則透過 KL散度最小化學習語意對應" },
        { "key": "D", "value": "三者在多模態應用中皆依賴同一潛在表徵空間（Shared Latent Space），僅在解碼器結構不同而已" }
      ],
      "answer": "A",
      "explanation": "選項 A 準確描述了三者的核心特質：VAE（模糊但結構好）、GAN（清晰但不穩）、Diffusion（清晰且穩定）。",
      "score": 2,
      "tags": ["生成模型", "深度學習"]
    },
    {
      "id": "q41",
      "type": "single_choice",
      "questionText": "在進行超參數調校（Hyperparameter Tuning）時，若直接在 K-Fold 交叉驗證（Cross-Validation）的資料上同時調整模型參數並評估效能，最可能導致下列哪一種問題？",
      "options": [
        { "key": "A", "value": "模型的交叉驗證結果出現過度樂觀偏差（Over-optimistic Bias），因測試摺資料間接參與參數選擇，造成資料洩漏（Data Leakage）" },
        { "key": "B", "value": "模型會在每一摺（Fold）內反覆調整參數，導致訓練不穩與過度正則化" },
        { "key": "C", "value": "因交叉驗證資料被重複使用，造成效能方差增大，無法獲得穩定估計" },
        { "key": "D", "value": "K-Fold 交叉驗證的假設與超參數搜尋相衝突，導致驗證過程失效" }
      ],
      "answer": "A",
      "explanation": "若使用同一組 CV 來選擇參數並評估效能，等於「用考試題目來複習」，會導致評估結果虛高（Optimistic Bias）。應使用巢狀交叉驗證（Nested CV）。",
      "score": 2,
      "tags": ["模型評估", "資料洩漏"]
    },
    {
      "id": "q42",
      "type": "single_choice",
      "questionText": "若部署一個深度學習模型至金融風控系統，該模型採用鑑別式架構（如 Transformer Classifier）。然而上線後，模型對新樣本的分類錯誤率顯著上升，經檢查發現，輸入資料分佈已與原訓練集明顯不同。針對此情形，下列哪一種應對策略最為適合？",
      "options": [
        { "key": "A", "value": "改用生成對抗網路（GAN）生成新樣本並混入訓練集" },
        { "key": "B", "value": "改用邏輯迴歸模型（Logistic Regression）以提升穩定性" },
        { "key": "C", "value": "增加模型容量（Model Capacity），以學習更多樣本差異" },
        { "key": "D", "value": "使用變分自編碼器（VAE）監控潛在空間分佈，偵測輸入資料偏移" }
      ],
      "answer": "D",
      "explanation": "面對分佈外（OOD）數據，VAE 可以透過計算重建誤差（Reconstruction Error）或潛在空間分佈來有效偵測異常或資料偏移。",
      "score": 2,
      "tags": ["異常偵測", "VAE"]
    },
    {
      "id": "q43",
      "type": "single_choice",
      "questionText": "某金融科技公司欲導入 AI模型協助客服郵件自動分類（投訴、詢問、表揚）。團隊同時考慮兩種模型設計：方案 A（生成式路徑）：採用 VAE建構潛在語意空間，再結合下游分類器進行標籤預測；方案 B（鑑別式路徑）：採用 BERT Classifier 直接根據輸入文本進行監督式分類。現有標註資料約 2,000 筆，資料分佈均勻但擴充成本高。若團隊希望公平比較兩種模型的資料利用效率與泛化能力，下列哪一種實驗設計最能突顯兩者的本質差異？",
      "options": [
        { "key": "A", "value": "在完整資料集上分別訓練兩者，並比較其分類準確率（Accuracy）與推論時間" },
        { "key": "B", "value": "在低資源情境（Low-resource Setting）下，逐步減少標註比例（100%、50%、10%），比較其 F1-score" },
        { "key": "C", "value": "使用 GAN 自動生成文本樣本補足資料，觀察兩模型在資料增強後的精確率（Precision）差異" },
        { "key": "D", "value": "在相同訓練資料上固定輸入維度，僅調整模型參數量，比較其對過擬合的敏感度" }
      ],
      "answer": "B",
      "explanation": "生成式與鑑別式模型的最大差異在於對數據的需求量。透過「減少標註比例」實驗，可以觀察哪種架構在少樣本（Low-resource）下仍能維持較好效能。",
      "score": 2,
      "tags": ["模型評估", "實驗設計"]
    },
    {
      "id": "q44",
      "type": "single_choice",
      "questionText": "某電信公司希望建立一個模型來預測顧客是否即將流失，並進一步模擬不同促銷或服務策略下顧客的行為變化，以生成多樣化的虛擬樣本資料進行 A/B 測試與行銷策略評估。若要同時兼顧預測與資料生成的需求，最適合採用下列哪一種方法？",
      "options": [
        { "key": "A", "value": "使用傳統隨機森林（Random Forest）" },
        { "key": "B", "value": "使用邏輯迴歸（Logistic Regression）模型" },
        { "key": "C", "value": "使用變分自編碼器（Variational Autoencoder, VAE）或生成對抗網路（Generative Adversarial Network, GAN）" },
        { "key": "D", "value": "使用強化學習代理（Reinforcement Learning Agent）" }
      ],
      "answer": "C",
      "explanation": "VAE 與 GAN 是生成模型，具備學習數據分佈並生成新樣本的能力，適合用於模擬與生成虛擬顧客行為數據。",
      "score": 2,
      "tags": ["生成模型", "VAE/GAN"]
    },
    {
      "id": "q45",
      "type": "single_choice",
      "questionText": "進行影像分類任務時，研究團隊嘗試利用主成分分析（Principal Component Analysis, PCA）將輸入特徵從 1024維降至 100維，並將降維後的資料輸入支持向量機（Support Vector Machine, SVM）模型進行訓練。關於此作法，下列哪一項描述最為合理？",
      "options": [
        { "key": "A", "value": "PCA保留的主成分必然能提升 SVM的分類準確率" },
        { "key": "B", "value": "使用原始高維資料通常更能保留資訊，因此 PCA沒有實際意義" },
        { "key": "C", "value": "PCA可讓 SVM自動適用於非線性（Nonlinear）資料集" },
        { "key": "D", "value": "降維後可降低訓練時間並減少過擬合（Overfitting）風險" }
      ],
      "answer": "D",
      "explanation": "PCA 能去除雜訊並減少特徵數量，這能顯著降低 SVM 的運算負擔（訓練時間）並減少因維度詛咒（Curse of Dimensionality）導致的過擬合。",
      "score": 2,
      "tags": ["降維", "PCA"]
    },
    {
      "id": "q46",
      "type": "single_choice",
      "questionText": "某企業的 AI 模型已部署於線上服務環境中，用於即時預測顧客流失機率。近期團隊注意到模型預測準確率逐漸下降，但系統運作正常且未出現錯誤訊息。經分析發現，近期輸入資料的分布與模型訓練資料相比出現顯著偏移。若要在 MLOps流程中主動偵測並預警此類問題，最應採用下列哪項措施？",
      "options": [
        { "key": "A", "value": "建立即時的資料漂移（Data Drift）與概念漂移（Concept Drift）監測機制" },
        { "key": "B", "value": "將模型轉換為量化版本以降低延遲" },
        { "key": "C", "value": "增加模型超參數調整次數以強化適應性" },
        { "key": "D", "value": "使用固定隨機種子（Random Seed）確保訓練穩定" }
      ],
      "answer": "A",
      "explanation": "模型效能隨時間下降且無系統錯誤，通常是 Data Drift 或 Concept Drift 所致。建立監控機制是 MLOps 的標準應對措施。",
      "score": 2,
      "tags": ["MLOps", "Data Drift"]
    },
    {
      "id": "q47",
      "type": "single_choice",
      "questionText": "某金融科技公司導入多任務學習架構，讓單一 Transformer 模型同時執行 OCR（Optical Character Recognition）後的文檔分類以及命名實體辨識（Named Entity Recognition, NER）任務，以協助自動歸檔與抽取關鍵金融資訊。在部署初期，團隊發現當模型的 NER準確率（Accuracy）提升時，文檔分類準確率反而下降。若模型架構正確且資料品質良好，下列哪一項最可能是造成此現象的原因？",
      "options": [
        { "key": "A", "value": "模型架構無法同時支援文字分類與序列標註任務（Sequence Labeling）" },
        { "key": "B", "value": "文檔分類任務不需要語意化表徵（Contextualized Representation）" },
        { "key": "C", "value": "損失函數（Loss Function）未進行權重平衡，導致任務間競爭" },
        { "key": "D", "value": "所使用的 BERT模型無法支援多任務輸出頭（Multi-Head Outputs）" }
      ],
      "answer": "C",
      "explanation": "多任務學習（Multi-task Learning）常面臨任務競爭。若損失函數未平衡權重（例如 NER Loss 遠大於分類 Loss），模型會傾向優化主導任務而犧牲其他任務。",
      "score": 2,
      "tags": ["深度學習", "多任務學習"]
    },
    {
      "id": "q48",
      "type": "single_choice",
      "questionText": "某數據工程師使用 DBSCAN演算法對一份數百萬筆的高維顧客資料進行聚類分析，但發現程式執行速度極慢，甚至出現記憶體不足的情況。若要在不改變演算法核心邏輯的前提下，最有效提升其運算效率的作法為何？",
      "options": [
        { "key": "A", "value": "改用以平均連結（Average Linkage）為基礎的階層式群集法（Hierarchical Clustering）" },
        { "key": "B", "value": "採用高效率的距離索引結構（Distance Index Structure），例如 KD-Tree 或 Ball Tree" },
        { "key": "C", "value": "將 ε（Epsilon）參數調得極小，以減少鄰近點的數量" },
        { "key": "D", "value": "在資料前處理時增加標準化後的特徵維度數" }
      ],
      "answer": "B",
      "explanation": "DBSCAN 的瓶頸在於計算鄰居距離。使用 KD-Tree 或 Ball Tree 等空間索引結構可以加速最近鄰搜尋，顯著提升效能。",
      "score": 2,
      "tags": ["分群", "演算法優化"]
    },
    {
      "id": "q49",
      "type": "single_choice",
      "questionText": "某電商平台導入 AI情感分析模型，用以自動偵測顧客評論中的負面情緒並觸發客服機制。然而，上線後發現模型在面對不同語言或族群書寫風格的評論時表現不一致，例如部分語氣強烈的正面評論被誤判為負面，而禮貌但含批評意圖的評論卻被判為中性。若從技術與資料治理的角度分析，下列哪一項描述不正確？",
      "options": [
        { "key": "A", "value": "模型未啟用詞嵌入正規化（Embedding Normalization）可能造成語意距離不穩定，導致預測誤差" },
        { "key": "B", "value": "訓練語料若偏向特定文化或語氣特徵，可能使模型產生內隱偏誤（Implicit Bias）" },
        { "key": "C", "value": "模型若訓練資料來源不平衡，容易導致對不同語言或族群風格的情緒判斷不準確" },
        { "key": "D", "value": "Transformer 架構能捕捉上下文語意，但若訓練資料偏差仍存在，模型仍可能學習到偏誤判斷" }
      ],
      "answer": "A",
      "explanation": "選項 A 是不正確的描述（本題為反向選擇）。雖然 Normalization 重要，但「語氣風格不一致」的主因通常是 B/C/D 所述的訓練資料偏差或文化偏誤，而非單純的 Embedding 數值正規化問題。",
      "score": 2,
      "tags": ["NLP", "AI倫理"]
    },
    {
      "id": "q50",
      "type": "single_choice",
      "questionText": "某設計師使用公司內部建置的生成式 AI工具製作行銷素材，並輸入提示語（Prompt）：「請生成一張模特兒手持品牌飲料、背景為海邊夕陽的照片」。系統能正確生成主要主題與場景，但輸出的圖像中，品牌標誌顏色常有誤差，或人物手部姿勢顯得不自然。若從多模態生成模型的技術機制分析，此現象最可能是下列哪一項原因所造成？",
      "options": [
        { "key": "A", "value": "擴散式生成模型的去雜訊過程出現隨機梯度漂移，導致影像像素錯誤" },
        { "key": "B", "value": "提示語過長造成 Transformer 的位置編碼超出上下文限制，導致生成混亂" },
        { "key": "C", "value": "CLIP 模型中的文字編碼器與影像編碼器在語意嵌入空間未充分對齊，導致跨模態理解偏差" },
        { "key": "D", "value": "模型未採用對比學習（Contrastive Learning）損失函數，無法建立多模態語意關聯" }
      ],
      "answer": "C",
      "explanation": "手部細節與特定 LOGO 的生成錯誤，常源於 CLIP Embedding 的對齊精度不足（CLIP 主要對齊整體語意而非局部細節），或是生成模型本身的歸納偏差（Inductive Bias）。選項 C 最符合技術描述。",
      "score": 2,
      "tags": ["生成式AI", "多模態"]
    }
  ]
}